{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6f64b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30466f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import CONFIGS\n",
    "from models.mlp import MLP\n",
    "\n",
    "N_SAMPLES_FOR_LOSS = 10000\n",
    "N_COMPONENTS_TO_ANALYZE = 50\n",
    "\n",
    "def calculate_subset_loss(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += criterion(output, target).item() * data.size(0)\n",
    "    return total_loss / len(data_loader.sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f9d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading data, model, and PCA results...\")\n",
    "try:\n",
    "    mean_weight = np.load('data/mean_weight.npy')\n",
    "    pca_components = np.load('data/pca_components.npy')\n",
    "    variances = np.load('data/pca_variances.npy')\n",
    "    model_state_dict = torch.load('data/final_model.pth')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e.filename} not found.\")\n",
    "    print(\"Please run 'train_model.py' and 'run_pca.py' first.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba04b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST('./mnist_data', train=True, download=True, transform=transform)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "indices = np.random.permutation(len(train_dataset))[:N_SAMPLES_FOR_LOSS]\n",
    "\n",
    "subset_sampler = SubsetRandomSampler(indices)\n",
    "sub_loader = DataLoader(train_dataset, batch_size=CONFIGS[\"batch_size\"], sampler=subset_sampler)\n",
    "\n",
    "model = MLP(input_dim=28*28, hidden_dim=50, output_dim=10).to(CONFIGS[\"device\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Calculating baseline loss L0 on the data subset...\")\n",
    "w0_tensor = torch.tensor(mean_weight.reshape(CONFIGS[\"hidden_dim\"], CONFIGS[\"hidden_dim\"]),\n",
    "                         dtype=torch.float32).to(CONFIGS[\"device\"])\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.fc2.weight.data = w0_tensor\n",
    "\n",
    "L0 = calculate_subset_loss(model, sub_loader, criterion, CONFIGS[\"device\"])\n",
    "flatness_threshold = L0 * np.e\n",
    "print(f\"Baseline Loss L0 (on subset): {L0:.6f}\")\n",
    "print(f\"Flatness Threshold (L0 * e): {flatness_threshold:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808cfcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_10 = pca_components[10]\n",
    "p_10_tensor = torch.tensor(p_10.reshape(CONFIGS[\"hidden_dim\"], CONFIGS[\"hidden_dim\"]),\n",
    "                              dtype=torch.float32).to(CONFIGS[\"device\"])\n",
    "\n",
    "p_20 = pca_components[20]\n",
    "p_20_tensor = torch.tensor(p_20.reshape(CONFIGS[\"hidden_dim\"], CONFIGS[\"hidden_dim\"]),\n",
    "                              dtype=torch.float32).to(CONFIGS[\"device\"])\n",
    "\n",
    "p_50 = pca_components[50]\n",
    "p_50_tensor = torch.tensor(p_50.reshape(CONFIGS[\"hidden_dim\"], CONFIGS[\"hidden_dim\"]),\n",
    "                              dtype=torch.float32).to(CONFIGS[\"device\"])\n",
    "\n",
    "p_100 = pca_components[100]\n",
    "p_100_tensor = torch.tensor(p_100.reshape(CONFIGS[\"hidden_dim\"], CONFIGS[\"hidden_dim\"]),\n",
    "                              dtype=torch.float32).to(CONFIGS[\"device\"])\n",
    "\n",
    "loss_r_10 = []\n",
    "loss_l_10 = []\n",
    "\n",
    "loss_r_20 = []\n",
    "loss_l_20 = []\n",
    "\n",
    "loss_r_50 = []\n",
    "loss_l_50 = []\n",
    "\n",
    "loss_r_100 = []\n",
    "loss_l_100 = []\n",
    "\n",
    "for step in np.linspace(0, 10, 50):\n",
    "    model.fc2.weight.data = w0_tensor + step * p_10_tensor\n",
    "    loss = calculate_subset_loss(model, sub_loader, criterion, CONFIGS[\"device\"])\n",
    "    loss_r_10.append(loss)\n",
    "\n",
    "    model.fc2.weight.data = w0_tensor + step * p_20_tensor\n",
    "    loss = calculate_subset_loss(model, sub_loader, criterion, CONFIGS[\"device\"])\n",
    "    loss_r_20.append(loss)\n",
    "\n",
    "    model.fc2.weight.data = w0_tensor + step * p_50_tensor\n",
    "    loss = calculate_subset_loss(model, sub_loader, criterion, CONFIGS[\"device\"])\n",
    "    loss_r_50.append(loss)\n",
    "\n",
    "    model.fc2.weight.data = w0_tensor + step * p_100_tensor\n",
    "    loss = calculate_subset_loss(model, sub_loader, criterion, CONFIGS[\"device\"])\n",
    "    loss_r_100.append(loss)\n",
    "\n",
    "for step in np.linspace(-10, 0, 50):\n",
    "    model.fc2.weight.data = w0_tensor - step * p_10_tensor\n",
    "    loss = calculate_subset_loss(model, sub_loader, criterion, CONFIGS[\"device\"])\n",
    "    loss_l_10.append(loss)\n",
    "\n",
    "    model.fc2.weight.data = w0_tensor - step * p_20_tensor\n",
    "    loss = calculate_subset_loss(model, sub_loader, criterion, CONFIGS[\"device\"])\n",
    "    loss_l_20.append(loss)\n",
    "\n",
    "    model.fc2.weight.data = w0_tensor - step * p_50_tensor\n",
    "    loss = calculate_subset_loss(model, sub_loader, criterion, CONFIGS[\"device\"])\n",
    "    loss_l_50.append(loss)\n",
    "\n",
    "    model.fc2.weight.data = w0_tensor - step * p_100_tensor\n",
    "    loss = calculate_subset_loss(model, sub_loader, criterion, CONFIGS[\"device\"])\n",
    "    loss_l_100.append(loss)\n",
    "\n",
    "loss_r_10 = np.array(loss_r_10)\n",
    "loss_l_10 = np.array(loss_l_10)\n",
    "loss_r_20 = np.array(loss_r_20)\n",
    "loss_l_20 = np.array(loss_l_20)\n",
    "loss_r_50 = np.array(loss_r_50)\n",
    "loss_l_50 = np.array(loss_l_50)\n",
    "loss_r_100 = np.array(loss_r_100)\n",
    "loss_l_100 = np.array(loss_l_100)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "steps = np.linspace(-10, 10, 100)\n",
    "plt.plot(steps, np.concatenate((loss_l_10, loss_r_10)), label='PCA Component 10')\n",
    "plt.plot(steps, np.concatenate((loss_l_20, loss_r_20)), label='PCA Component 20')\n",
    "plt.plot(steps, np.concatenate((loss_l_50, loss_r_50)), label='PCA Component 50')\n",
    "plt.plot(steps, np.concatenate((loss_l_100, loss_r_100)), label='PCA Component 100')\n",
    "plt.axhline(y=flatness_threshold, color='r', linestyle='--', label='Flatness Threshold (L0 * e)')\n",
    "plt.xlabel('Step Size along PCA Component')\n",
    "plt.ylabel('Loss on Subset')\n",
    "plt.title('Loss Landscape along PCA Directions')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('loss_landscape_pca_directions.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0148ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatness_values = []\n",
    "print(f\"\\nMeasuring flatness for the top {N_COMPONENTS_TO_ANALYZE} PCA directions...\")\n",
    "\n",
    "for i in tqdm(range(N_COMPONENTS_TO_ANALYZE), desc=\"Analyzing Directions\"):\n",
    "    p_i = pca_components[i]\n",
    "    p_i_tensor = torch.tensor(p_i.reshape(CONFIGS[\"hidden_dim\"], CONFIGS[\"hidden_dim\"]),\n",
    "                              dtype=torch.float32).to(CONFIGS[\"device\"])\n",
    "\n",
    "    delta_theta_r = 0.0\n",
    "    for step in np.logspace(-2, 1.5, 50):\n",
    "        model.fc2.weight.data = w0_tensor + step * p_i_tensor\n",
    "        loss = calculate_subset_loss(model, sub_loader, criterion, CONFIGS[\"device\"])\n",
    "        if loss > flatness_threshold:\n",
    "            delta_theta_r = step\n",
    "            break\n",
    "\n",
    "    delta_theta_l = 0.0\n",
    "    for step in np.logspace(-2, 1.5, 50):\n",
    "        model.fc2.weight.data = w0_tensor - step * p_i_tensor\n",
    "        loss = calculate_subset_loss(model, sub_loader, criterion, CONFIGS[\"device\"])\n",
    "        if loss > flatness_threshold:\n",
    "            delta_theta_l = -step\n",
    "            break\n",
    "        \n",
    "    flatness = delta_theta_r - delta_theta_l\n",
    "    flatness_values.append(flatness)\n",
    "\n",
    "flatness_values = np.array(flatness_values)\n",
    "\n",
    "print(\"\\nAnalysis complete. Generating plots...\")\n",
    "if not os.path.exists('plots'):\n",
    "    os.makedirs('plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b1cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/flatness_values.npy', flatness_values)\n",
    "\n",
    "indices_analyzed = np.arange(1, N_COMPONENTS_TO_ANALYZE + 1)\n",
    "variances_analyzed = variances[:N_COMPONENTS_TO_ANALYZE]\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(indices_analyzed, variances_analyzed, 'o-')\n",
    "plt.yscale('log'); plt.xscale('log')\n",
    "plt.title('Variance vs. PC Index'); plt.xlabel('PC Index (i)'); plt.ylabel('Variance (σ²)')\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(indices_analyzed, flatness_values, 'o-')\n",
    "plt.title('Flatness vs. PC Index'); plt.xlabel('PC Index (i)'); plt.ylabel('Flatness (F)')\n",
    "plt.yscale('log'); plt.xscale('log')\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "# Filter out any zero or negative flatness values before log transform for fitting\n",
    "valid_mask = flatness_values > 0\n",
    "if np.sum(valid_mask) > 1:\n",
    "    log_F = np.log(flatness_values[valid_mask])\n",
    "    log_var = np.log(variances_analyzed[valid_mask])\n",
    "    m, c = np.polyfit(log_F, log_var, 1)\n",
    "    plt.plot(flatness_values[valid_mask], np.exp(m * log_F + c), 'r--', label=f'Fit (slope ≈ {m:.2f})')\n",
    "\n",
    "plt.plot(flatness_values, variances_analyzed, 'o', label='Experimental Data')\n",
    "plt.title('Variance vs. Flatness'); plt.xlabel('Flatness (F)'); plt.ylabel('Variance (σ²)')\n",
    "plt.yscale('log'); plt.xscale('log')\n",
    "plt.legend(); plt.grid(True, which=\"both\", ls=\"--\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/feng_tu_2021_reproduction.png')\n",
    "print(\"Saved plot to 'plots/feng_tu_2021_reproduction.png'\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
